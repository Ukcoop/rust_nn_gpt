#version 450
#extension GL_EXT_shader_atomic_float : require

layout(local_size_x = 256) in;

layout(set = 0, binding = 0) buffer GradAttnOutBuffer { float grad_attn_out[]; };
layout(set = 0, binding = 1) buffer SoftmaxBuffer { float softmax[]; };
layout(set = 0, binding = 2) buffer VBuffer { float v[]; };
layout(set = 0, binding = 3) buffer GradSoftmaxBuffer { float grad_softmax[]; };
layout(set = 0, binding = 4) buffer GradVBuffer { float grad_v[]; };

layout(push_constant) uniform PushConstants {
    uint batch_size;
    uint seq_len;
    uint num_heads;
    uint head_dim;
    uint embed_dim;
} pc;

void main() {
    uint global_id = gl_GlobalInvocationID.x;
    
    // Calculate indices for output gradients
    uint total_output_elements = pc.batch_size * pc.num_heads * pc.seq_len * pc.head_dim;
    if (global_id >= total_output_elements) return;
    
    uint output_idx = global_id;
    uint batch = output_idx / (pc.num_heads * pc.seq_len * pc.head_dim);
    uint remaining = output_idx % (pc.num_heads * pc.seq_len * pc.head_dim);
    uint head = remaining / (pc.seq_len * pc.head_dim);
    uint i = (remaining % (pc.seq_len * pc.head_dim)) / pc.head_dim;  // query position
    uint d = remaining % pc.head_dim;  // head dimension
    
    // Get gradient of attention output
    float grad_attn_out_val = grad_attn_out[output_idx];
    
    // Compute gradients for softmax weights and values
    for (uint j = 0; j < pc.seq_len; j++) {
        // Get attention weight for this query-key pair
        uint weight_idx = batch * pc.num_heads * pc.seq_len * pc.seq_len + 
                         head * pc.seq_len * pc.seq_len + 
                         i * pc.seq_len + j;
        float attention_weight = softmax[weight_idx];
        
        // Get value for this key position
        uint v_offset = batch * pc.seq_len * pc.embed_dim + 
                       j * pc.embed_dim + 
                       head * pc.head_dim + d;
        float value = v[v_offset];
        
        // Gradient for softmax weight: ∂L/∂softmax[i,j] = ∂L/∂attn_out[i,d] * v[j,d]
        atomicAdd(grad_softmax[weight_idx], grad_attn_out_val * value);
        
        // Gradient for value: ∂L/∂v[j,d] = ∂L/∂attn_out[i,d] * softmax[i,j]
        atomicAdd(grad_v[v_offset], grad_attn_out_val * attention_weight);
    }
} 