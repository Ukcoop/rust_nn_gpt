#version 450
#extension GL_EXT_shader_atomic_float : require

layout(local_size_x = 256) in;

layout(set = 0, binding = 0) buffer GradScoresBuffer { float grad_scores[]; };
layout(set = 0, binding = 1) buffer QBuffer { float q[]; };
layout(set = 0, binding = 2) buffer KBuffer { float k[]; };
layout(set = 0, binding = 3) buffer GradQBuffer { float grad_q[]; };
layout(set = 0, binding = 4) buffer GradKBuffer { float grad_k[]; };

layout(push_constant) uniform PushConstants {
    uint batch_size;
    uint seq_len;
    uint num_heads;
    uint head_dim;
    uint embed_dim;
} pc;

void main() {
    uint global_id = gl_GlobalInvocationID.x;
    
    // Calculate indices
    uint total_attention_scores = pc.batch_size * pc.num_heads * pc.seq_len * pc.seq_len;
    if (global_id >= total_attention_scores) return;
    
    uint score_idx = global_id;
    uint batch = score_idx / (pc.num_heads * pc.seq_len * pc.seq_len);
    uint remaining = score_idx % (pc.num_heads * pc.seq_len * pc.seq_len);
    uint head = remaining / (pc.seq_len * pc.seq_len);
    uint i = (remaining % (pc.seq_len * pc.seq_len)) / pc.seq_len;  // query position
    uint j = remaining % pc.seq_len;  // key position
    
    // Get gradient of attention score
    float grad_score = grad_scores[score_idx];
    
    // Calculate offsets for Q and K
    uint q_offset = batch * pc.seq_len * pc.embed_dim + i * pc.embed_dim + head * pc.head_dim;
    uint k_offset = batch * pc.seq_len * pc.embed_dim + j * pc.embed_dim + head * pc.head_dim;
    
    // Compute gradients for Q and K
    // ∂score/∂q[i,d] = k[j,d] / sqrt(head_dim)
    // ∂score/∂k[j,d] = q[i,d] / sqrt(head_dim)
    float scale = 1.0 / sqrt(float(pc.head_dim));
    for (uint d = 0; d < pc.head_dim; d++) {
        float k_val = k[k_offset + d];
        float q_val = q[q_offset + d];
        
        // Gradient for Q
        atomicAdd(grad_q[q_offset + d], grad_score * k_val * scale);
        
        // Gradient for K
        atomicAdd(grad_k[k_offset + d], grad_score * q_val * scale);
    }
} 