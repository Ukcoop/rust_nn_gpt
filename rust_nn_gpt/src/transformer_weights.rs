use serde::{Serialize, Deserialize};

#[derive(Serialize, Deserialize, Clone)]
pub struct TransformerWeights {
    pub input_proj_weight: Vec<Vec<f32>>,
    pub input_proj_bias: Vec<f32>,
    pub input_proj_m_weight: Vec<Vec<f32>>,
    pub input_proj_v_weight: Vec<Vec<f32>>,
    pub input_proj_m_bias: Vec<f32>,
    pub input_proj_v_bias: Vec<f32>,
    pub input_proj_t: usize,
    pub input_norm_gamma: Vec<f32>,
    pub input_norm_beta: Vec<f32>,
    pub input_norm_m_gamma: Vec<f32>,
    pub input_norm_v_gamma: Vec<f32>,
    pub input_norm_m_beta: Vec<f32>,
    pub input_norm_v_beta: Vec<f32>,
    pub input_norm_t: usize,
    pub blocks: Vec<BlockWeights>,
    pub output_norm_gamma: Vec<f32>,
    pub output_norm_beta: Vec<f32>,
    pub output_norm_m_gamma: Vec<f32>,
    pub output_norm_v_gamma: Vec<f32>,
    pub output_norm_m_beta: Vec<f32>,
    pub output_norm_v_beta: Vec<f32>,
    pub output_norm_t: usize,
    pub output_proj_weight: Vec<Vec<f32>>,
    pub output_proj_bias: Vec<f32>,
    pub output_proj_m_weight: Vec<Vec<f32>>,
    pub output_proj_v_weight: Vec<Vec<f32>>,
    pub output_proj_m_bias: Vec<f32>,
    pub output_proj_v_bias: Vec<f32>,
    pub output_proj_t: usize,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct BlockWeights {
    // MultiHeadAttention weights
    pub attn_wq_weight: Vec<Vec<f32>>,
    pub attn_wq_bias: Vec<f32>,
    pub attn_wq_m_weight: Vec<Vec<f32>>,
    pub attn_wq_v_weight: Vec<Vec<f32>>,
    pub attn_wq_m_bias: Vec<f32>,
    pub attn_wq_v_bias: Vec<f32>,
    pub attn_wq_t: usize,
    pub attn_wk_weight: Vec<Vec<f32>>,
    pub attn_wk_bias: Vec<f32>,
    pub attn_wk_m_weight: Vec<Vec<f32>>,
    pub attn_wk_v_weight: Vec<Vec<f32>>,
    pub attn_wk_m_bias: Vec<f32>,
    pub attn_wk_v_bias: Vec<f32>,
    pub attn_wk_t: usize,
    pub attn_wv_weight: Vec<Vec<f32>>,
    pub attn_wv_bias: Vec<f32>,
    pub attn_wv_m_weight: Vec<Vec<f32>>,
    pub attn_wv_v_weight: Vec<Vec<f32>>,
    pub attn_wv_m_bias: Vec<f32>,
    pub attn_wv_v_bias: Vec<f32>,
    pub attn_wv_t: usize,
    pub attn_wo_weight: Vec<Vec<f32>>,
    pub attn_wo_bias: Vec<f32>,
    pub attn_wo_m_weight: Vec<Vec<f32>>,
    pub attn_wo_v_weight: Vec<Vec<f32>>,
    pub attn_wo_m_bias: Vec<f32>,
    pub attn_wo_v_bias: Vec<f32>,
    pub attn_wo_t: usize,
    // LayerNorms
    pub norm1_gamma: Vec<f32>,
    pub norm1_beta: Vec<f32>,
    pub norm1_m_gamma: Vec<f32>,
    pub norm1_v_gamma: Vec<f32>,
    pub norm1_m_beta: Vec<f32>,
    pub norm1_v_beta: Vec<f32>,
    pub norm1_t: usize,
    pub norm2_gamma: Vec<f32>,
    pub norm2_beta: Vec<f32>,
    pub norm2_m_gamma: Vec<f32>,
    pub norm2_v_gamma: Vec<f32>,
    pub norm2_m_beta: Vec<f32>,
    pub norm2_v_beta: Vec<f32>,
    pub norm2_t: usize,
    // FeedForward
    pub ff_linear1_weight: Vec<Vec<f32>>,
    pub ff_linear1_bias: Vec<f32>,
    pub ff_linear1_m_weight: Vec<Vec<f32>>,
    pub ff_linear1_v_weight: Vec<Vec<f32>>,
    pub ff_linear1_m_bias: Vec<f32>,
    pub ff_linear1_v_bias: Vec<f32>,
    pub ff_linear1_t: usize,
    pub ff_linear2_weight: Vec<Vec<f32>>,
    pub ff_linear2_bias: Vec<f32>,
    pub ff_linear2_m_weight: Vec<Vec<f32>>,
    pub ff_linear2_v_weight: Vec<Vec<f32>>,
    pub ff_linear2_m_bias: Vec<f32>,
    pub ff_linear2_v_bias: Vec<f32>,
    pub ff_linear2_t: usize,
} 